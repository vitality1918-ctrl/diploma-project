{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1Tri5d9zFNz"
      },
      "source": [
        "# –§–∏–Ω–∞–ª—å–Ω—ã–π –Ω–æ—É—Ç–±—É–∫\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# –Ø–ß–ï–ô–ö–ê 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∏ –∏–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
        "# ============================================================================\n",
        "print(\"=\" * 80)\n",
        "print(\"–°–ò–°–¢–ï–ú–ê NER –î–õ–Ø –Æ–†–ò–î–ò–ß–ï–°–ö–ò–• –î–û–ö–£–ú–ï–ù–¢–û–í\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫ (–µ—Å–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã)\n",
        "try:\n",
        "    import torch\n",
        "except ImportError:\n",
        "    !pip install -q torch numpy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "BASE_DIR = os.path.dirname(os.path.abspath(\"__file__\")) \\\n",
        "    if \"__file__\" in globals() else os.getcwd()\n",
        "\n",
        "os.chdir(BASE_DIR)\n",
        "\n",
        "from datetime import datetime\n",
        "from IPython.display import HTML, display\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# –Ø–ß–ï–ô–ö–ê 2: –ö–û–ù–°–¢–ê–ù–¢–´ –ò –ù–ê–°–¢–†–û–ô–ö–ò (–≤—ã–Ω–µ—Å–µ–Ω—ã –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é)\n",
        "# ============================================================================\n",
        "class Config:\n",
        "    \"\"\"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã NER\"\"\"\n",
        "    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
        "    MAX_LEN = 256  # –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
        "    STRIDE = 128   # –®–∞–≥ —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –æ–∫–Ω–∞\n",
        "\n",
        "    # –¶–≤–µ—Ç–∞ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π\n",
        "    LABEL_COLORS = {\n",
        "        \"s1\": \"#1976d2\",  # —Å–∏–Ω–∏–π - –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –∏ –ø—Ä–∞–≤–∞\n",
        "        \"s2\": \"#388e3c\",  # –∑–µ–ª—ë–Ω—ã–π - –¥–µ–Ω—å–≥–∏, —Å—É–º–º—ã –∏ –ø—Ä–æ—Ü–µ–Ω—Ç—ã\n",
        "        \"s3\": \"#fbc02d\",  # –∂—ë–ª—Ç—ã–π - –¥–∞—Ç—ã –∏ –ø–µ—Ä–∏–æ–¥—ã\n",
        "        \"s4\": \"#d32f2f\",  # –∫—Ä–∞—Å–Ω—ã–π - –∞–¥—Ä–µ—Å–∞\n",
        "        \"O\": \"#ffffff\"    # –±–µ–ª—ã–π - –Ω–µ —Å—É—â–Ω–æ—Å—Ç—å\n",
        "    }\n",
        "\n",
        "    # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ, –≤ —Ç–æ–π –∂–µ –ø–∞–ø–∫–µ)\n",
        "    DEFAULT_PATHS = {\n",
        "        \"model\": \"bilstm_ner_best.pt\",           # –º–æ–¥–µ–ª—å\n",
        "        \"char_vocab\": \"char_vocab.json\",         # —Å–ª–æ–≤–∞—Ä—å —Å–∏–º–≤–æ–ª–æ–≤\n",
        "        \"label_vocab\": \"label_vocab.json\",       # —Å–ª–æ–≤–∞—Ä—å –º–µ—Ç–æ–∫\n",
        "        \"sample_text\": \"sample_contract.txt\"     # –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –¥–µ–º–æ\n",
        "    }\n",
        "\n",
        "    # –û–ø–∏—Å–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤\n",
        "    CLASS_DESCRIPTIONS = {\n",
        "        \"O\": \"–ù–µ —Å—É—â–Ω–æ—Å—Ç—å\",\n",
        "        \"s1\": \"–û–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞, –ø—Ä–∞–≤–∞, —Å—É–±—ä–µ–∫—Ç—ã\",\n",
        "        \"s2\": \"–î–µ–Ω—å–≥–∏, —Å—É–º–º—ã, –ø—Ä–æ—Ü–µ–Ω—Ç—ã\",\n",
        "        \"s3\": \"–î–∞—Ç—ã, —Å—Ä–æ–∫–∏, –ø–µ—Ä–∏–æ–¥—ã\",\n",
        "        \"s4\": \"–ê–¥—Ä–µ—Å–∞, –º–µ—Å—Ç–∞, –ª–æ–∫–∞—Ü–∏–∏\"\n",
        "    }\n",
        "\n",
        "    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏\n",
        "    EMBEDDING_DIM = 128\n",
        "    HIDDEN_DIM = 128\n",
        "\n",
        "config = Config()\n",
        "print(\"‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# –Ø–ß–ï–ô–ö–ê 3: –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ú–û–î–ï–õ–ò BiLSTM (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)\n",
        "# ============================================================================\n",
        "class BiLSTMNER(nn.Module):\n",
        "    \"\"\"BiLSTM –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (NER)\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_labels, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=pad_idx\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_labels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        out, _ = self.lstm(emb)\n",
        "        logits = self.fc(out)\n",
        "        return logits\n",
        "\n",
        "print(\"‚úÖ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# –Ø–ß–ï–ô–ö–ê 4: –§–£–ù–ö–¶–ò–Ø –ó–ê–ì–†–£–ó–ö–ò –ú–û–î–ï–õ–ò –ò –°–õ–û–í–ê–†–ï–ô (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞)\n",
        "# ============================================================================\n",
        "def load_model_and_vocabs(model_path=None, char_vocab_path=None, label_vocab_path=None):\n",
        "    \"\"\"\n",
        "    –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ —Å–ª–æ–≤–∞—Ä–µ–π\n",
        "\n",
        "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
        "    - model_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–æ–¥–µ–ª–∏ (.pt) –∏–ª–∏ None –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
        "    - char_vocab_path: –ø—É—Ç—å –∫ —Å–ª–æ–≤–∞—Ä—é —Å–∏–º–≤–æ–ª–æ–≤ (.json)\n",
        "    - label_vocab_path: –ø—É—Ç—å –∫ —Å–ª–æ–≤–∞—Ä—é –º–µ—Ç–æ–∫ (.json)\n",
        "\n",
        "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
        "    - model: –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å\n",
        "    - char2idx: —Å–ª–æ–≤–∞—Ä—å —Å–∏–º–≤–æ–ª->–∏–Ω–¥–µ–∫—Å\n",
        "    - idx2label: —Å–ª–æ–≤–∞—Ä—å –∏–Ω–¥–µ–∫—Å->–º–µ—Ç–∫–∞\n",
        "    - device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (cpu/cuda)\n",
        "    \"\"\"\n",
        "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã\n",
        "    if model_path is None:\n",
        "        model_path = config.DEFAULT_PATHS[\"model/bilstm_ner_best.pt\"]\n",
        "    if char_vocab_path is None:\n",
        "        char_vocab_path = config.DEFAULT_PATHS[\"vocab/char_vocab.json\"]\n",
        "    if label_vocab_path is None:\n",
        "        label_vocab_path = config.DEFAULT_PATHS[\"vocab/label_vocab.json\"]\n",
        "\n",
        "    print(f\"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑: {model_path}\")\n",
        "    print(f\"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑: {char_vocab_path}\")\n",
        "    print(f\"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è –º–µ—Ç–æ–∫ –∏–∑: {label_vocab_path}\")\n",
        "\n",
        "    try:\n",
        "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤\n",
        "        for path, name in [(model_path, \"–º–æ–¥–µ–ª—å\"),\n",
        "                          (char_vocab_path, \"—Å–ª–æ–≤–∞—Ä—å —Å–∏–º–≤–æ–ª–æ–≤\"),\n",
        "                          (label_vocab_path, \"—Å–ª–æ–≤–∞—Ä—å –º–µ—Ç–æ–∫\")]:\n",
        "            if not os.path.exists(path):\n",
        "                raise FileNotFoundError(f\"–§–∞–π–ª {name} –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}\")\n",
        "\n",
        "        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä–µ–π\n",
        "        with open(char_vocab_path, 'r', encoding='utf-8') as f:\n",
        "            char2idx = json.load(f)\n",
        "\n",
        "        with open(label_vocab_path, 'r', encoding='utf-8') as f:\n",
        "            label2idx = json.load(f)\n",
        "\n",
        "        # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è –º–µ—Ç–æ–∫\n",
        "        idx2label = {int(v): k for k, v in label2idx.items()}\n",
        "\n",
        "        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏\n",
        "        vocab_size = len(char2idx)\n",
        "        num_labels = len(label2idx)\n",
        "        pad_idx = char2idx.get('<PAD>', 0)\n",
        "\n",
        "        print(f\"üìä –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è —Å–∏–º–≤–æ–ª–æ–≤: {vocab_size}\")\n",
        "        print(f\"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {num_labels}\")\n",
        "\n",
        "        # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏\n",
        "        model = BiLSTMNER(\n",
        "            vocab_size=vocab_size,\n",
        "            embedding_dim=config.EMBEDDING_DIM,\n",
        "            hidden_dim=config.HIDDEN_DIM,\n",
        "            num_labels=num_labels,\n",
        "            pad_idx=pad_idx\n",
        "        )\n",
        "\n",
        "        # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        print(f\"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n",
        "        print(f\"‚úÖ –°–ª–æ–≤–∞—Ä–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
        "\n",
        "        return model, char2idx, idx2label, device\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω - {e}\")\n",
        "        print(\"üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Å–ª–µ–¥—É—é—â–∏–µ —Ñ–∞–π–ª—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Ç–æ–π –∂–µ –ø–∞–ø–∫–µ, —á—Ç–æ –∏ –Ω–æ—É—Ç–±—É–∫:\")\n",
        "        for key, filename in config.DEFAULT_PATHS.items():\n",
        "            if key != \"sample_text\":  # sample_text –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π\n",
        "                print(f\"   - {filename}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}\")\n",
        "        raise\n",
        "\n",
        "print(\"‚úÖ –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# –Ø–ß–ï–ô–ö–ê 5: –§–£–ù–ö–¶–ò–Ø –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ò –¢–ï–ö–°–¢–ê (—á–∏—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è)\n",
        "# ============================================================================\n",
        "def preprocess_text(text, lower=True):\n",
        "    \"\"\"–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –º–æ–¥–µ–ª–∏\"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
        "    if lower:\n",
        "        text = text.lower()\n",
        "\n",
        "    # –ó–∞–º–µ–Ω–∞ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫ –Ω–∞ –ø—Ä–æ–±–µ–ª—ã\n",
        "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
        "\n",
        "    # –£–¥–∞–ª–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "print(\"‚úÖ –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# –Ø–ß–ï–ô–ö–ê 6: –§–£–ù–ö–¶–ò–Ø –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–ò (—á–∏—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è)\n",
        "# ============================================================================\n",
        "def text_to_indices(text, char2idx, max_len=None):\n",
        "    \"\"\"\n",
        "    –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –º–æ–¥–µ–ª–∏\n",
        "\n",
        "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
        "    - text: –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
        "    - char2idx: —Å–ª–æ–≤–∞—Ä—å —Å–∏–º–≤–æ–ª->–∏–Ω–¥–µ–∫—Å\n",
        "    - max_len: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ config)\n",
        "\n",
        "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
        "    - indices: —Å–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤\n",
        "    - chars: —Å–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤\n",
        "    \"\"\"\n",
        "    if max_len is None:\n",
        "        max_len = config.MAX_LEN\n",
        "\n",
        "    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å–∏–º–≤–æ–ª—ã\n",
        "    chars = list(text)\n",
        "\n",
        "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–∏–º–≤–æ–ª—ã –≤ –∏–Ω–¥–µ–∫—Å—ã\n",
        "    unk_idx = char2idx.get('<UNK>', 0)\n",
        "    pad_idx = char2idx.get('<PAD>', 0)\n",
        "\n",
        "    indices = [char2idx.get(c, unk_idx) for c in chars]\n",
        "\n",
        "    # –û–±—Ä–µ–∑–∞–µ–º –¥–æ max_len\n",
        "    indices = indices[:max_len]\n",
        "    chars = chars[:max_len]\n",
        "\n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞–¥–¥–∏–Ω–≥ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n",
        "    if len(indices) < max_len:\n",
        "        indices += [pad_idx] * (max_len - len(indices))\n",
        "        chars += ['<PAD>'] * (max_len - len(chars))\n",
        "\n",
        "    return indices, chars\n",
        "\n",
        "print(\"‚úÖ –§—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# –Ø–ß–ï–ô–ö–ê 7: –§–£–ù–ö–¶–ò–Ø –°–û–ó–î–ê–ù–ò–Ø –°–ö–û–õ–¨–ó–Ø–©–ï–ì–û –û–ö–ù–ê (—á–∏—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è)\n",
        "# ============================================================================\n",
        "def create_sliding_windows(text, window_size=None, stride=None):\n",
        "    \"\"\"\n",
        "    –°–æ–∑–¥–∞–Ω–∏–µ —Å–∫–æ–ª—å–∑—è—â–∏—Ö –æ–∫–æ–Ω –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤\n",
        "\n",
        "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
        "    - text: –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
        "    - window_size: —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ config)\n",
        "    - stride: —à–∞–≥ –æ–∫–Ω–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ config)\n",
        "\n",
        "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
        "    - windows: —Å–ø–∏—Å–æ–∫ –æ–∫–æ–Ω (–ø–æ–¥—Å—Ç—Ä–æ–∫)\n",
        "    - starts: –Ω–∞—á–∞–ª—å–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –æ–∫–æ–Ω\n",
        "    \"\"\"\n",
        "    if window_size is None:\n",
        "        window_size = config.MAX_LEN\n",
        "    if stride is None:\n",
        "        stride = config.STRIDE\n",
        "\n",
        "    windows = []\n",
        "    starts = []\n",
        "\n",
        "    for start in range(0, len(text), stride):\n",
        "        window = text[start:start + window_size]\n",
        "        if window:  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–ø—É—Å—Ç—ã–µ –æ–∫–Ω–∞\n",
        "            windows.append(window)\n",
        "            starts.append(start)\n",
        "\n",
        "    print(f\"üìä –°–æ–∑–¥–∞–Ω–æ {len(windows)} –æ–∫–æ–Ω (—Ä–∞–∑–º–µ—Ä: {window_size}, —à–∞–≥: {stride})\")\n",
        "    return windows, starts\n",
        "\n",
        "print(\"‚úÖ –§—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –æ–∫–Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# –Ø–ß–ï–ô–ö–ê 8: –§–£–ù–ö–¶–ò–Ø –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø (—á–∏—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è)\n",
        "# ============================================================================\n",
        "def predict_entities(model, text, char2idx, idx2label, device):\n",
        "    \"\"\"\n",
        "    –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è —Ç–µ–∫—Å—Ç–∞\n",
        "\n",
        "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
        "    - model: –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å\n",
        "    - text: –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
        "    - char2idx: —Å–ª–æ–≤–∞—Ä—å —Å–∏–º–≤–æ–ª->–∏–Ω–¥–µ–∫—Å\n",
        "    - idx2label: —Å–ª–æ–≤–∞—Ä—å –∏–Ω–¥–µ–∫—Å->–º–µ—Ç–∫–∞\n",
        "    - device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (CPU/GPU)\n",
        "\n",
        "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
        "    - pred_labels: —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞\n",
        "    \"\"\"\n",
        "    # –°–æ–∑–¥–∞–µ–º —Å–∫–æ–ª—å–∑—è—â–∏–µ –æ–∫–Ω–∞\n",
        "    windows, starts = create_sliding_windows(text)\n",
        "\n",
        "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–∞—Å—Å–∏–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
        "    pred_labels = ['O'] * len(text)\n",
        "\n",
        "    print(f\"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª–∏–Ω–æ–π {len(text)} —Å–∏–º–≤–æ–ª–æ–≤...\")\n",
        "\n",
        "    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥–æ–µ –æ–∫–Ω–æ\n",
        "    for i, (window, start) in enumerate(zip(windows, starts)):\n",
        "        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ–∫–Ω–æ –≤ –∏–Ω–¥–µ–∫—Å—ã\n",
        "        indices, _ = text_to_indices(window, char2idx)\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º —Ç–µ–Ω–∑–æ—Ä\n",
        "        x_tensor = torch.tensor([indices], dtype=torch.long).to(device)\n",
        "\n",
        "        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "        with torch.no_grad():\n",
        "            logits = model(x_tensor)\n",
        "            predictions = torch.argmax(logits, dim=-1)[0].cpu().numpy()\n",
        "\n",
        "        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –≤ –º–µ—Ç–∫–∏\n",
        "        window_labels = [idx2label[int(pred)] for pred in predictions[:len(window)]]\n",
        "\n",
        "        # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "        for j, label in enumerate(window_labels):\n",
        "            pos = start + j\n",
        "            if pos < len(pred_labels):\n",
        "                if pred_labels[pos] == 'O' and label != 'O':\n",
        "                    pred_labels[pos] = label\n",
        "\n",
        "    return pred_labels\n",
        "\n",
        "print(\"‚úÖ –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# –Ø–ß–ï–ô–ö–ê 9: –§–£–ù–ö–¶–ò–Ø –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò –†–ï–ó–£–õ–¨–¢–ê–¢–û–í (—á–∏—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è)\n",
        "# ============================================================================\n",
        "def visualize_results_html(text, pred_labels):\n",
        "    \"\"\"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å —Ü–≤–µ—Ç–æ–≤—ã–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≤ HTML\"\"\"\n",
        "    if not text or not pred_labels:\n",
        "        return \"<p>–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏</p>\"\n",
        "\n",
        "    html_parts = []\n",
        "    current_word = []\n",
        "    current_labels = []\n",
        "\n",
        "    for char, label in zip(text, pred_labels):\n",
        "        if char == ' ':\n",
        "            if current_word:\n",
        "                word_text = ''.join(current_word)\n",
        "                word_label = next((l for l in current_labels if l != 'O'), 'O')\n",
        "                color = config.LABEL_COLORS.get(word_label, '#ffffff')\n",
        "\n",
        "                if word_label == 'O':\n",
        "                    style = f\"color:#000000;\"\n",
        "                else:\n",
        "                    style = f\"background-color:{color};color:#ffffff;padding:2px 4px;border-radius:4px;font-weight:bold;\"\n",
        "                    word_text = f\"[{word_label[1:]}] {word_text}\"\n",
        "\n",
        "                html_parts.append(f'<span style=\"{style}\">{word_text}</span> ')\n",
        "                current_word = []\n",
        "                current_labels = []\n",
        "            else:\n",
        "                html_parts.append(' ')\n",
        "        else:\n",
        "            current_word.append(char)\n",
        "            current_labels.append(label)\n",
        "\n",
        "    if current_word:\n",
        "        word_text = ''.join(current_word)\n",
        "        word_label = next((l for l in current_labels if l != 'O'), 'O')\n",
        "        color = config.LABEL_COLORS.get(word_label, '#ffffff')\n",
        "\n",
        "        if word_label == 'O':\n",
        "            style = f\"color:#000000;\"\n",
        "        else:\n",
        "            style = f\"background-color:{color};color:#ffffff;padding:2px 4px;border-radius:4px;font-weight:bold;\"\n",
        "            word_text = f\"[{word_label[1:]}] {word_text}\"\n",
        "\n",
        "        html_parts.append(f'<span style=\"{style}\">{word_text}</span>')\n",
        "\n",
        "    result_html = '<div style=\"font-family: monospace; line-height: 1.8; padding: 20px; background-color: #f5f5f5; border-radius: 10px;\">'\n",
        "    result_html += '<h3 style=\"color: #333;\">–†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π:</h3>'\n",
        "    result_html += '<div style=\"background-color: white; padding: 20px; border-radius: 8px; border: 1px solid #ddd;\">'\n",
        "    result_html += ''.join(html_parts)\n",
        "    result_html += '</div>'\n",
        "\n",
        "    result_html += '<div style=\"margin-top: 20px; padding: 15px; background-color: white; border-radius: 8px; border: 1px solid #ddd;\">'\n",
        "    result_html += '<h4 style=\"color: #333; margin-bottom: 10px;\">–õ–µ–≥–µ–Ω–¥–∞:</h4>'\n",
        "    result_html += '<div style=\"display: flex; flex-wrap: wrap; gap: 10px;\">'\n",
        "\n",
        "    for label, color in config.LABEL_COLORS.items():\n",
        "        if label != 'O':\n",
        "            desc = config.CLASS_DESCRIPTIONS.get(label, label)\n",
        "            result_html += f'<div style=\"display: inline-block; padding: 5px 10px; background-color: {color}; color: white; border-radius: 4px; font-weight: bold;\">{label}: {desc}</div>'\n",
        "\n",
        "    result_html += '</div></div></div>'\n",
        "\n",
        "    return result_html\n",
        "\n",
        "def visualize_results_console(text, pred_labels):\n",
        "    \"\"\"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∫–æ–Ω—Å–æ–ª–∏\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"–†–ï–ó–£–õ–¨–¢–ê–¢ –†–ê–°–ü–û–ó–ù–ê–í–ê–ù–ò–Ø –°–£–©–ù–û–°–¢–ï–ô\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    entities = defaultdict(list)\n",
        "    current_entity = None\n",
        "    current_text = []\n",
        "    start_pos = 0\n",
        "\n",
        "    for i, (char, label) in enumerate(zip(text, pred_labels)):\n",
        "        if label != 'O':\n",
        "            if label != current_entity:\n",
        "                if current_entity is not None and current_text:\n",
        "                    entities[current_entity].append({\n",
        "                        'text': ''.join(current_text),\n",
        "                        'start': start_pos,\n",
        "                        'end': i\n",
        "                    })\n",
        "                current_entity = label\n",
        "                current_text = [char]\n",
        "                start_pos = i\n",
        "            else:\n",
        "                current_text.append(char)\n",
        "        else:\n",
        "            if current_entity is not None and current_text:\n",
        "                entities[current_entity].append({\n",
        "                    'text': ''.join(current_text),\n",
        "                    'start': start_pos,\n",
        "                    'end': i\n",
        "                })\n",
        "                current_entity = None\n",
        "                current_text = []\n",
        "\n",
        "    if current_entity is not None and current_text:\n",
        "        entities[current_entity].append({\n",
        "            'text': ''.join(current_text),\n",
        "            'start': start_pos,\n",
        "            'end': len(text)\n",
        "        })\n",
        "\n",
        "    print(f\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è:\")\n",
        "    print(f\"   –í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤: {len(text)}\")\n",
        "\n",
        "    total_entities = 0\n",
        "    for label, entity_list in entities.items():\n",
        "        count = len(entity_list)\n",
        "        total_entities += count\n",
        "        desc = config.CLASS_DESCRIPTIONS.get(label, label)\n",
        "        print(f\"   {label} ({desc}): {count} —Å—É—â–Ω–æ—Å—Ç–µ–π\")\n",
        "\n",
        "    print(f\"   –í—Å–µ–≥–æ —Å—É—â–Ω–æ—Å—Ç–µ–π: {total_entities}\")\n",
        "\n",
        "    if total_entities > 0:\n",
        "        print(f\"\\nüîç –ü—Ä–∏–º–µ—Ä—ã –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π:\")\n",
        "        for label, entity_list in entities.items():\n",
        "            if entity_list:\n",
        "                desc = config.CLASS_DESCRIPTIONS.get(label, label)\n",
        "                print(f\"\\n   {label} ({desc}):\")\n",
        "                for i, entity in enumerate(entity_list[:3]):\n",
        "                    print(f\"     {i+1}. '{entity['text']}' (–ø–æ–∑–∏—Ü–∏–∏ {entity['start']}-{entity['end']})\")\n",
        "                if len(entity_list) > 3:\n",
        "                    print(f\"     ... –∏ –µ—â–µ {len(entity_list) - 3} —Å—É—â–Ω–æ—Å—Ç–µ–π\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è –°—É—â–Ω–æ—Å—Ç–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã\")\n",
        "\n",
        "    print(f\"\\nüé® –¶–≤–µ—Ç–æ–≤–∞—è –∫–æ–¥–∏—Ä–æ–≤–∫–∞:\")\n",
        "    for label, color in config.LABEL_COLORS.items():\n",
        "        if label != 'O':\n",
        "            desc = config.CLASS_DESCRIPTIONS.get(label, label)\n",
        "            print(f\"   {color} {label}: {desc}\")\n",
        "\n",
        "print(\"‚úÖ –§—É–Ω–∫—Ü–∏–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# –Ø–ß–ï–ô–ö–ê 10: –§–£–ù–ö–¶–ò–Ø –°–û–•–†–ê–ù–ï–ù–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –æ—à–∏–±–∫–∞ —Å datetime)\n",
        "# ============================================================================\n",
        "def save_results(text, pred_labels, output_path=\"ner_results.txt\"):\n",
        "    \"\"\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ñ–∞–π–ª\"\"\"\n",
        "    try:\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"=\" * 80 + \"\\n\")\n",
        "            f.write(\"–†–ï–ó–£–õ–¨–¢–ê–¢–´ –†–ê–°–ü–û–ó–ù–ê–í–ê–ù–ò–Ø –°–£–©–ù–û–°–¢–ï–ô\\n\")\n",
        "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "\n",
        "            f.write(f\"–î–∞—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "            f.write(f\"–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤\\n\\n\")\n",
        "\n",
        "            entities = defaultdict(list)\n",
        "            current_entity = None\n",
        "            current_text = []\n",
        "            start_pos = 0\n",
        "\n",
        "            for i, (char, label) in enumerate(zip(text, pred_labels)):\n",
        "                if label != 'O':\n",
        "                    if label != current_entity:\n",
        "                        if current_entity is not None and current_text:\n",
        "                            entities[current_entity].append({\n",
        "                                'text': ''.join(current_text),\n",
        "                                'start': start_pos,\n",
        "                                'end': i\n",
        "                            })\n",
        "                        current_entity = label\n",
        "                        current_text = [char]\n",
        "                        start_pos = i\n",
        "                    else:\n",
        "                        current_text.append(char)\n",
        "                else:\n",
        "                    if current_entity is not None and current_text:\n",
        "                        entities[current_entity].append({\n",
        "                            'text': ''.join(current_text),\n",
        "                            'start': start_pos,\n",
        "                            'end': i\n",
        "                        })\n",
        "                        current_entity = None\n",
        "                        current_text = []\n",
        "\n",
        "            if current_entity is not None and current_text:\n",
        "                entities[current_entity].append({\n",
        "                    'text': ''.join(current_text),\n",
        "                    'start': start_pos,\n",
        "                    'end': len(text)\n",
        "                })\n",
        "\n",
        "            f.write(\"–°–¢–ê–¢–ò–°–¢–ò–ö–ê:\\n\")\n",
        "            f.write(\"-\" * 40 + \"\\n\")\n",
        "            total_entities = 0\n",
        "            for label, entity_list in entities.items():\n",
        "                count = len(entity_list)\n",
        "                total_entities += count\n",
        "                desc = config.CLASS_DESCRIPTIONS.get(label, label)\n",
        "                f.write(f\"{label}: {desc} - {count} —Å—É—â–Ω–æ—Å—Ç–µ–π\\n\")\n",
        "            f.write(f\"–í—Å–µ–≥–æ —Å—É—â–Ω–æ—Å—Ç–µ–π: {total_entities}\\n\\n\")\n",
        "\n",
        "            if total_entities > 0:\n",
        "                f.write(\"–ù–ê–ô–î–ï–ù–ù–´–ï –°–£–©–ù–û–°–¢–ò:\\n\")\n",
        "                f.write(\"-\" * 40 + \"\\n\")\n",
        "                for label, entity_list in entities.items():\n",
        "                    if entity_list:\n",
        "                        desc = config.CLASS_DESCRIPTIONS.get(label, label)\n",
        "                        f.write(f\"\\n{label} ({desc}):\\n\")\n",
        "                        for i, entity in enumerate(entity_list, 1):\n",
        "                            f.write(f\"  {i:3d}. '{entity['text']}' (–ø–æ–∑–∏—Ü–∏–∏ {entity['start']}-{entity['end']})\\n\")\n",
        "\n",
        "            f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "            f.write(\"–¢–ï–ö–°–¢ –° –¶–í–ï–¢–û–í–û–ô –†–ê–ó–ú–ï–¢–ö–û–ô (HTML)\\n\")\n",
        "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "            f.write(visualize_results_html(text, pred_labels))\n",
        "\n",
        "        print(f\"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {output_path}\")\n",
        "        return output_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ –§—É–Ω–∫—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# –Ø–ß–ï–ô–ö–ê 11: –§–£–ù–ö–¶–ò–Ø –ó–ê–ì–†–£–ó–ö–ò –§–ê–ô–õ–ê (—Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Google Colab)\n",
        "# ============================================================================\n",
        "def load_text_from_file(file_path=None):\n",
        "    \"\"\"\n",
        "    –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞\n",
        "\n",
        "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
        "    - file_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É\n",
        "\n",
        "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
        "    - –¢–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ\n",
        "    \"\"\"\n",
        "    if file_path is None:\n",
        "        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞\n",
        "        sample_path = config.DEFAULT_PATHS[\"data/sample_contract.txt\"]\n",
        "        if os.path.exists(sample_path):\n",
        "            print(f\"üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞: {sample_path}\")\n",
        "            file_path = sample_path\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω. –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –≤—Ä—É—á–Ω—É—é.\")\n",
        "            return None\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "        print(f\"‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: {file_path}\")\n",
        "        print(f\"üìÑ –†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
        "        return text\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# –Ø–ß–ï–ô–ö–ê 12: –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –ó–ê–ì–†–£–ó–ö–ò –§–ê–ô–õ–û–í –í COLAB (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
        "# ============================================================================\n",
        "def upload_files_colab():\n",
        "    \"\"\"–§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤ –≤ Google Colab\"\"\"\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        print(\"üìÅ –ó–∞–≥—Ä—É–∑–∏—Ç–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã (–º–æ–¥–µ–ª—å –∏ —Å–ª–æ–≤–∞—Ä–∏):\")\n",
        "\n",
        "        # –°–ø–∏—Å–æ–∫ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ–∞–π–ª–æ–≤\n",
        "        required_files = [\n",
        "            \"bilstm_ner_best.pt\",\n",
        "            \"char_vocab.json\",\n",
        "            \"label_vocab.json\"\n",
        "        ]\n",
        "\n",
        "        print(\"–ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã:\")\n",
        "        for file in required_files:\n",
        "            print(f\"  - {file}\")\n",
        "\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        if uploaded:\n",
        "            print(\"‚úÖ –§–∞–π–ª—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã:\")\n",
        "            for filename in uploaded.keys():\n",
        "                print(f\"  - {filename}\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è –§–∞–π–ª—ã –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
        "            return False\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"‚ÑπÔ∏è Google Colab –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã.\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤: {e}\")\n",
        "        return False\n",
        "\n",
        "print(\"‚úÖ –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤ –≤ Colab –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# –Ø–ß–ï–ô–ö–ê 13: –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –ò–ù–§–ï–†–ï–ù–°–ê (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞)\n",
        "# ============================================================================\n",
        "def run_ner_inference(input_text=None, input_file=None,\n",
        "                      model_path=None, char_vocab_path=None, label_vocab_path=None,\n",
        "                      save_output=True, visualize=True):\n",
        "    \"\"\"\n",
        "    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ NER –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\n",
        "\n",
        "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
        "    - input_text: —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Å—Ç—Ä–æ–∫–∞)\n",
        "    - input_file: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —Ç–µ–∫—Å—Ç–æ–º\n",
        "    - model_path: –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏\n",
        "    - char_vocab_path: –ø—É—Ç—å –∫ —Å–ª–æ–≤–∞—Ä—é —Å–∏–º–≤–æ–ª–æ–≤\n",
        "    - label_vocab_path: –ø—É—Ç—å –∫ —Å–ª–æ–≤–∞—Ä—é –º–µ—Ç–æ–∫\n",
        "    - save_output: —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª\n",
        "    - visualize: –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –ª–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é\n",
        "\n",
        "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
        "    - results: —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üöÄ –ó–ê–ü–£–°–ö NER –ò–ù–§–ï–†–ï–ù–°–ê\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    results = None\n",
        "\n",
        "    try:\n",
        "        # 1. –ü–û–õ–£–ß–ï–ù–ò–ï –í–•–û–î–ù–´–• –î–ê–ù–ù–´–•\n",
        "        print(\"\\n1. üì• –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...\")\n",
        "\n",
        "        text = None\n",
        "\n",
        "        if input_text:\n",
        "            text = input_text\n",
        "            print(\"   –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç\")\n",
        "        elif input_file:\n",
        "            text = load_text_from_file(input_file)\n",
        "        else:\n",
        "            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ —Ñ–∞–π–ª–∞ –∏–ª–∏ –∑–∞–ø—Ä–æ—Å–∏—Ç—å\n",
        "            text = load_text_from_file()\n",
        "            if text is None:\n",
        "                print(\"   –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (Ctrl+Enter –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è):\")\n",
        "                text = input()\n",
        "\n",
        "        if not text or len(text.strip()) == 0:\n",
        "            print(\"‚ùå –¢–µ–∫—Å—Ç –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω\")\n",
        "            return None\n",
        "\n",
        "        print(f\"   –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
        "\n",
        "        # 2. –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò –ò –°–õ–û–í–ê–†–ï–ô\n",
        "        print(\"\\n2. ü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Å–ª–æ–≤–∞—Ä–µ–π...\")\n",
        "        model, char2idx, idx2label, device = load_model_and_vocabs(\n",
        "            model_path=model_path,\n",
        "            char_vocab_path=char_vocab_path,\n",
        "            label_vocab_path=label_vocab_path\n",
        "        )\n",
        "\n",
        "        # 3. –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –¢–ï–ö–°–¢–ê\n",
        "        print(\"\\n3. üîß –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞...\")\n",
        "        processed_text = preprocess_text(text, lower=True)\n",
        "        print(f\"   –¢–µ–∫—Å—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω: {len(processed_text)} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
        "\n",
        "        # 4. –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï –°–£–©–ù–û–°–¢–ï–ô\n",
        "        print(\"\\n4. üîÆ –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π...\")\n",
        "        pred_labels = predict_entities(model, processed_text, char2idx, idx2label, device)\n",
        "\n",
        "        # 5. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í\n",
        "        if visualize:\n",
        "            print(\"\\n5. üé® –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...\")\n",
        "            visualize_results_console(processed_text, pred_labels)\n",
        "            html_result = visualize_results_html(processed_text, pred_labels)\n",
        "            display(HTML(html_result))\n",
        "\n",
        "        # 6. –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í\n",
        "        output_file = None\n",
        "        if save_output:\n",
        "            print(\"\\n6. üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...\")\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            output_file = f\"ner_results_{timestamp}.txt\"\n",
        "            save_results(processed_text, pred_labels, output_file)\n",
        "\n",
        "        # 7. –í–û–ó–í–†–ê–¢ –†–ï–ó–£–õ–¨–¢–ê–¢–û–í\n",
        "        results = {\n",
        "            'text': text,\n",
        "            'processed_text': processed_text,\n",
        "            'predictions': pred_labels,\n",
        "            'entities_count': sum(1 for label in pred_labels if label != 'O'),\n",
        "            'output_file': output_file\n",
        "        }\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"‚úÖ –ò–ù–§–ï–†–ï–ù–° –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        return results\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"\\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}\")\n",
        "        print(\"\\nüí° –†–ï–®–ï–ù–ò–ï –ü–†–û–ë–õ–ï–ú–´:\")\n",
        "        print(\"1. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Å–ª–µ–¥—É—é—â–∏–µ —Ñ–∞–π–ª—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Ç–æ–π –∂–µ –ø–∞–ø–∫–µ, —á—Ç–æ –∏ –Ω–æ—É—Ç–±—É–∫:\")\n",
        "        for key, filename in config.DEFAULT_PATHS.items():\n",
        "            if key != \"sample_text\":\n",
        "                print(f\"   - {filename}\")\n",
        "        print(\"\\n2. –ò–ª–∏ —É–∫–∞–∂–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø—É—Ç–∏ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ —Ñ—É–Ω–∫—Ü–∏–∏:\")\n",
        "        print('   run_ner_inference(input_text=\"–í–∞—à —Ç–µ–∫—Å—Ç\", model_path=\"–ø—É—Ç—å/–∫/–º–æ–¥–µ–ª–∏.pt\", ...)')\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå –û–®–ò–ë–ö–ê –í–´–ü–û–õ–ù–ï–ù–ò–Ø: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# –Ø–ß–ï–ô–ö–ê 14: –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò\n",
        "# ============================================================================\n",
        "def check_required_files():\n",
        "    \"\"\"–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ–∞–π–ª–æ–≤\"\"\"\n",
        "    print(\"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ–∞–π–ª–æ–≤...\")\n",
        "\n",
        "    missing_files = []\n",
        "\n",
        "    for key, filename in config.DEFAULT_PATHS.items():\n",
        "        if key != \"sample_text\":  # sample_text –æ–ø—Ü–∏–æ–Ω–∞–ª–µ–Ω\n",
        "            if os.path.exists(filename):\n",
        "                print(f\"   ‚úÖ {filename} - –Ω–∞–π–¥–µ–Ω\")\n",
        "            else:\n",
        "                print(f\"   ‚ùå {filename} - –û–¢–°–£–¢–°–¢–í–£–ï–¢\")\n",
        "                missing_files.append(filename)\n",
        "\n",
        "    if missing_files:\n",
        "        print(f\"\\n‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç {len(missing_files)} —Ñ–∞–π–ª–æ–≤:\")\n",
        "        for file in missing_files:\n",
        "            print(f\"   - {file}\")\n",
        "        print(\"\\nüí° –†–µ—à–µ–Ω–∏—è:\")\n",
        "        print(\"1. –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ñ–∞–π–ª—ã –≤ —Ç—É –∂–µ –ø–∞–ø–∫—É, –≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–æ—É—Ç–±—É–∫\")\n",
        "        print(\"2. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é upload_files_colab() –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Google Colab\")\n",
        "        print(\"3. –£–∫–∞–∂–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø—É—Ç–∏ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ run_ner_inference()\")\n",
        "        return False\n",
        "    else:\n",
        "        print(\"\\n‚úÖ –í—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç!\")\n",
        "        return True\n",
        "\n",
        "def run_demo_example():\n",
        "    \"\"\"–ó–∞–ø—É—Å–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üß™ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–û–ù–ù–´–ô –ü–†–ò–ú–ï–†\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    demo_text = \"\"\"–î–æ–≥–æ–≤–æ—Ä ‚Ññ123 –æ—Ç 10 —è–Ω–≤–∞—Ä—è 2024 –≥–æ–¥–∞.\n",
        "–°—É–º–º–∞ –¥–æ–≥–æ–≤–æ—Ä–∞: 150000 —Ä—É–±–ª–µ–π.\n",
        "–ê–≤–∞–Ω—Å 20% (30000 —Ä—É–±–ª–µ–π) –æ–ø–ª–∞—á–∏–≤–∞–µ—Ç—Å—è –≤ —Ç–µ—á–µ–Ω–∏–µ 5 –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –¥–Ω–µ–π.\n",
        "–û—Å—Ç–∞–≤—à–∞—è—Å—è —Å—É–º–º–∞ 120000 —Ä—É–±–ª–µ–π –æ–ø–ª–∞—á–∏–≤–∞–µ—Ç—Å—è –¥–æ 30 –º–∞—Ä—Ç–∞ 2024 –≥–æ–¥–∞.\n",
        "–ê–¥—Ä–µ—Å –ø–æ—Å—Ç–∞–≤–∫–∏: –≥. –ú–æ—Å–∫–≤–∞, —É–ª. –õ–µ–Ω–∏–Ω–∞, –¥. 15.\"\"\"\n",
        "\n",
        "    print(\"–¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(demo_text)\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    results = run_ner_inference(\n",
        "        input_text=demo_text,\n",
        "        save_output=False,\n",
        "        visualize=True\n",
        "    )\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"‚úÖ –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# –Ø–ß–ï–ô–ö–ê 15: –¢–ï–°–¢–û–í–ê–Ø –Ø–ß–ï–ô–ö–ê –î–õ–Ø –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø (–§–ò–ù–ê–õ–¨–ù–ê–Ø)\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"üéØ –°–ò–°–¢–ï–ú–ê NER –î–õ–Ø –Æ–†–ò–î–ò–ß–ï–°–ö–ò–• –î–û–ö–£–ú–ï–ù–¢–û–í - –ì–û–¢–û–í–ê –ö –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤\n",
        "files_ok = check_required_files()\n",
        "\n",
        "if files_ok:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üöÄ –°–ò–°–¢–ï–ú–ê –ì–û–¢–û–í–ê –ö –†–ê–ë–û–¢–ï!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # –ó–∞–ø—É—Å–∫–∞–µ–º –¥–µ–º–æ\n",
        "    print(\"\\n–•–æ—Ç–∏—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä? (y/n): \", end=\"\")\n",
        "    choice = input().strip().lower()\n",
        "\n",
        "    if choice == 'y':\n",
        "        run_demo_example()\n",
        "else:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"‚ö†Ô∏è  –ù–ï–û–ë–•–û–î–ò–ú–û –ù–ê–°–¢–†–û–ò–¢–¨ –°–ò–°–¢–ï–ú–£\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—Ö–æ–¥–∏–º—Å—è –ª–∏ –≤ Google Colab\n",
        "    try:\n",
        "        import google.colab\n",
        "        is_colab = True\n",
        "    except:\n",
        "        is_colab = False\n",
        "\n",
        "    if is_colab:\n",
        "        print(\"\\n–û–±–Ω–∞—Ä—É–∂–µ–Ω Google Colab\")\n",
        "        print(\"–•–æ—Ç–∏—Ç–µ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª—ã? (y/n): \", end=\"\")\n",
        "        choice = input().strip().lower()\n",
        "\n",
        "        if choice == 'y':\n",
        "            upload_files_colab()\n",
        "            # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞\n",
        "            if check_required_files():\n",
        "                run_demo_example()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìù –ò–ù–°–¢–†–£–ö–¶–ò–Ø –ü–û –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "print(\"1. –ë–´–°–¢–†–´–ô –°–¢–ê–†–¢ (–µ—Å–ª–∏ –≤—Å–µ —Ñ–∞–π–ª—ã –Ω–∞ –º–µ—Å—Ç–µ):\")\n",
        "print('   results = run_ner_inference(input_text=\"–í–∞—à —Ç–µ–∫—Å—Ç –¥–æ–≥–æ–≤–æ—Ä–∞\")')\n",
        "print()\n",
        "print(\"2. –° –ó–ê–ì–†–£–ó–ö–û–ô –ò–ó –§–ê–ô–õ–ê:\")\n",
        "print(\"   results = run_ner_inference(input_file='–¥–æ–≥–æ–≤–æ—Ä.txt')\")\n",
        "print()\n",
        "print(\"3. –° –£–ö–ê–ó–ê–ù–ò–ï–ú –ü–£–¢–ï–ô –ö –§–ê–ô–õ–ê–ú:\")\n",
        "print('   results = run_ner_inference(')\n",
        "print('       input_text=\"–¢–µ–∫—Å—Ç\",')\n",
        "print('       model_path=\"–º–æ–¥–µ–ª—å.pt\",')\n",
        "print('       char_vocab_path=\"char_vocab.json\",')\n",
        "print('       label_vocab_path=\"label_vocab.json\"')\n",
        "print('   )')\n",
        "print()\n",
        "print(\"4. –ë–ï–ó –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò (—Ç–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã):\")\n",
        "print('   results = run_ner_inference(input_text=\"–¢–µ–∫—Å—Ç\", visualize=False)')\n",
        "print()\n",
        "print(\"5. –ü–†–û–í–ï–†–ö–ê –§–ê–ô–õ–û–í:\")\n",
        "print(\"   check_required_files()\")\n",
        "print()\n",
        "print(\"6. –î–ï–ú–û-–ü–†–ò–ú–ï–†:\")\n",
        "print(\"   run_demo_example()\")\n",
        "print()\n",
        "print(\"üìå –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ —Ñ–∞–π–ª ner_results_<–¥–∞—Ç–∞_–≤—Ä–µ–º—è>.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GvgrVaUVG9nC",
        "outputId": "15e0f234-e3cf-476d-a261-79680a4f6e9c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "–°–ò–°–¢–ï–ú–ê NER –î–õ–Ø –Æ–†–ò–î–ò–ß–ï–°–ö–ò–• –î–û–ö–£–ú–ï–ù–¢–û–í\n",
            "================================================================================\n",
            "‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n",
            "\n",
            "‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n",
            "\n",
            "‚úÖ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\n",
            "\n",
            "‚úÖ –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\n",
            "\n",
            "‚úÖ –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\n",
            "\n",
            "‚úÖ –§—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\n",
            "\n",
            "‚úÖ –§—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –æ–∫–Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\n",
            "\n",
            "‚úÖ –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\n",
            "\n",
            "‚úÖ –§—É–Ω–∫—Ü–∏–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\n",
            "\n",
            "‚úÖ –§—É–Ω–∫—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\n",
            "\n",
            "‚úÖ –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\n",
            "\n",
            "‚úÖ –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤ –≤ Colab –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\n",
            "\n",
            "‚úÖ –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\n",
            "\n",
            "‚úÖ –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\n",
            "\n",
            "================================================================================\n",
            "üéØ –°–ò–°–¢–ï–ú–ê NER –î–õ–Ø –Æ–†–ò–î–ò–ß–ï–°–ö–ò–• –î–û–ö–£–ú–ï–ù–¢–û–í - –ì–û–¢–û–í–ê –ö –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ\n",
            "================================================================================\n",
            "\n",
            "üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ–∞–π–ª–æ–≤...\n",
            "   ‚ùå bilstm_ner_best.pt - –û–¢–°–£–¢–°–¢–í–£–ï–¢\n",
            "   ‚ùå char_vocab.json - –û–¢–°–£–¢–°–¢–í–£–ï–¢\n",
            "   ‚ùå label_vocab.json - –û–¢–°–£–¢–°–¢–í–£–ï–¢\n",
            "\n",
            "‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç 3 —Ñ–∞–π–ª–æ–≤:\n",
            "   - bilstm_ner_best.pt\n",
            "   - char_vocab.json\n",
            "   - label_vocab.json\n",
            "\n",
            "üí° –†–µ—à–µ–Ω–∏—è:\n",
            "1. –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ñ–∞–π–ª—ã –≤ —Ç—É –∂–µ –ø–∞–ø–∫—É, –≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–æ—É—Ç–±—É–∫\n",
            "2. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é upload_files_colab() –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Google Colab\n",
            "3. –£–∫–∞–∂–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø—É—Ç–∏ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ run_ner_inference()\n",
            "\n",
            "================================================================================\n",
            "‚ö†Ô∏è  –ù–ï–û–ë–•–û–î–ò–ú–û –ù–ê–°–¢–†–û–ò–¢–¨ –°–ò–°–¢–ï–ú–£\n",
            "================================================================================\n",
            "\n",
            "–û–±–Ω–∞—Ä—É–∂–µ–Ω Google Colab\n",
            "–•–æ—Ç–∏—Ç–µ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª—ã? (y/n): Y\n",
            "üìÅ –ó–∞–≥—Ä—É–∑–∏—Ç–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã (–º–æ–¥–µ–ª—å –∏ —Å–ª–æ–≤–∞—Ä–∏):\n",
            "–ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã:\n",
            "  - bilstm_ner_best.pt\n",
            "  - char_vocab.json\n",
            "  - label_vocab.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d0768e79-2a67-452a-8266-8cc4bbdac2f9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d0768e79-2a67-452a-8266-8cc4bbdac2f9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving bilstm_ner_best (1).pt to bilstm_ner_best (1).pt\n",
            "‚úÖ –§–∞–π–ª—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã:\n",
            "  - bilstm_ner_best (1).pt\n",
            "üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ–∞–π–ª–æ–≤...\n",
            "   ‚ùå bilstm_ner_best.pt - –û–¢–°–£–¢–°–¢–í–£–ï–¢\n",
            "   ‚ùå char_vocab.json - –û–¢–°–£–¢–°–¢–í–£–ï–¢\n",
            "   ‚ùå label_vocab.json - –û–¢–°–£–¢–°–¢–í–£–ï–¢\n",
            "\n",
            "‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç 3 —Ñ–∞–π–ª–æ–≤:\n",
            "   - bilstm_ner_best.pt\n",
            "   - char_vocab.json\n",
            "   - label_vocab.json\n",
            "\n",
            "üí° –†–µ—à–µ–Ω–∏—è:\n",
            "1. –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ñ–∞–π–ª—ã –≤ —Ç—É –∂–µ –ø–∞–ø–∫—É, –≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–æ—É—Ç–±—É–∫\n",
            "2. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é upload_files_colab() –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Google Colab\n",
            "3. –£–∫–∞–∂–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø—É—Ç–∏ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ run_ner_inference()\n",
            "\n",
            "================================================================================\n",
            "üìù –ò–ù–°–¢–†–£–ö–¶–ò–Ø –ü–û –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ\n",
            "================================================================================\n",
            "\n",
            "1. –ë–´–°–¢–†–´–ô –°–¢–ê–†–¢ (–µ—Å–ª–∏ –≤—Å–µ —Ñ–∞–π–ª—ã –Ω–∞ –º–µ—Å—Ç–µ):\n",
            "   results = run_ner_inference(input_text=\"–í–∞—à —Ç–µ–∫—Å—Ç –¥–æ–≥–æ–≤–æ—Ä–∞\")\n",
            "\n",
            "2. –° –ó–ê–ì–†–£–ó–ö–û–ô –ò–ó –§–ê–ô–õ–ê:\n",
            "   results = run_ner_inference(input_file='–¥–æ–≥–æ–≤–æ—Ä.txt')\n",
            "\n",
            "3. –° –£–ö–ê–ó–ê–ù–ò–ï–ú –ü–£–¢–ï–ô –ö –§–ê–ô–õ–ê–ú:\n",
            "   results = run_ner_inference(\n",
            "       input_text=\"–¢–µ–∫—Å—Ç\",\n",
            "       model_path=\"–º–æ–¥–µ–ª—å.pt\",\n",
            "       char_vocab_path=\"char_vocab.json\",\n",
            "       label_vocab_path=\"label_vocab.json\"\n",
            "   )\n",
            "\n",
            "4. –ë–ï–ó –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò (—Ç–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã):\n",
            "   results = run_ner_inference(input_text=\"–¢–µ–∫—Å—Ç\", visualize=False)\n",
            "\n",
            "5. –ü–†–û–í–ï–†–ö–ê –§–ê–ô–õ–û–í:\n",
            "   check_required_files()\n",
            "\n",
            "6. –î–ï–ú–û-–ü–†–ò–ú–ï–†:\n",
            "   run_demo_example()\n",
            "\n",
            "üìå –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ —Ñ–∞–π–ª ner_results_<–¥–∞—Ç–∞_–≤—Ä–µ–º—è>.txt\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "fptCPUtZbNKA",
        "Fzc-1N2BZDyy",
        "afuIcuVZZMBI",
        "9peYOsQVZoW7",
        "M42mZHXqZ89W",
        "G4ld2kecYrFw",
        "KIdCAOsbcNNX",
        "6FI9t9jAY1XD"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}