{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1Tri5d9zFNz"
      },
      "source": [
        "# Этап 1. Unified pipeline: загрузка данных/словари, классы, функции (всё в одной ячейке).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# ЯЧЕЙКА 1: Unified pipeline\n",
        "# - загрузка vocab + sample данных\n",
        "# - Dataset / model / метрики / train_model / plot helpers\n",
        "# Все функции документированы комментариями\n",
        "# ==========================\n",
        "\n",
        "# --- Импорты и конфигурация ---\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Фиксируем seed для воспроизводимости\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "# (опционально) детерминируем cudnn (может замедлить)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# --- Пути к ресурсам (скачит из GitHub в рабочую директорию) ---\n",
        "os.makedirs(\"vocab\", exist_ok=True)\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# vocab (из твоего репозитория final_notebook)\n",
        "!wget -q -O vocab/char_vocab.json https://raw.githubusercontent.com/vitality1918-ctrl/diploma-project/main/final_notebook/vocab/char_vocab.json\n",
        "!wget -q -O vocab/label_vocab.json https://raw.githubusercontent.com/vitality1918-ctrl/diploma-project/main/final_notebook/vocab/label_vocab.json\n",
        "\n",
        "# sample data (предварительно положи xTrain_BiLSTM_idx_5000.txt и yTrain_BiLSTM_padded_5000.txt\n",
        "# в репозиторий experiments_notebook/data или замените ссылки ниже на свои raw ссылки)\n",
        "!wget -q -O data/xTrain_BiLSTM_idx_5000.txt https://raw.githubusercontent.com/vitality1918-ctrl/diploma-project/main/experiments_notebook/data/xTrain_BiLSTM_idx_5000.txt\n",
        "!wget -q -O data/yTrain_BiLSTM_padded_5000.txt https://raw.githubusercontent.com/vitality1918-ctrl/diploma-project/main/experiments_notebook/data/yTrain_BiLSTM_padded_5000.txt\n",
        "\n",
        "print(\"Files in data/:\", os.listdir(\"data\"))\n",
        "print(\"Files in vocab/:\", os.listdir(\"vocab\"))\n",
        "\n",
        "# --- Функция загрузки словарей ---\n",
        "def load_vocabs(char_vocab_path=\"vocab/char_vocab.json\", label_vocab_path=\"vocab/label_vocab.json\"):\n",
        "    \"\"\"\n",
        "    Загружает char_vocab и label_vocab из json-файлов.\n",
        "    Возвращает: char2idx (dict), label2idx (dict), idx2label (dict), pad_token_id, pad_label_id\n",
        "    \"\"\"\n",
        "    with open(char_vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        char2idx = json.load(f)\n",
        "    with open(label_vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        label2idx = json.load(f)\n",
        "\n",
        "    # приводим индексы в int, если они хранятся как строки\n",
        "    label2idx = {k: int(v) for k, v in label2idx.items()}\n",
        "    idx2label = {v: k for k, v in label2idx.items()}\n",
        "\n",
        "    pad_token_id = char2idx.get(\"<PAD>\", 0)\n",
        "    pad_label_id = label2idx.get(\"<PAD>\", None)\n",
        "    if pad_label_id is None:\n",
        "        # если нет PAD метки в словаре, используем -100 как ignore_index для CrossEntropyLoss\n",
        "        pad_label_id = -100\n",
        "\n",
        "    return char2idx, label2idx, idx2label, pad_token_id, pad_label_id\n",
        "\n",
        "# --- Dataset для indexed sequence файла ---\n",
        "class BiLSTMIndexedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Загружает x и y из файлов, где каждая строка - последовательность индексов, разделённых пробелом.\n",
        "    Возвращает тензоры одинаковой длины (предполагается, что в файлах уже есть padding до фиксированной длины).\n",
        "    \"\"\"\n",
        "    def __init__(self, x_path, y_path):\n",
        "        with open(x_path, \"r\", encoding=\"utf-8\") as fx:\n",
        "            x_lines = fx.readlines()\n",
        "        with open(y_path, \"r\", encoding=\"utf-8\") as fy:\n",
        "            y_lines = fy.readlines()\n",
        "        assert len(x_lines) == len(y_lines), \"x и y должны содержать одинаковое количество строк\"\n",
        "        self.x = [list(map(int, l.strip().split())) for l in x_lines]\n",
        "        self.y = [list(map(int, l.strip().split())) for l in y_lines]\n",
        "        # вычислим max_len (в sample все строки одинаковой длины обычно)\n",
        "        self.max_len = max(len(seq) for seq in self.x)\n",
        "        print(f\"[Dataset] Loaded {len(self.x)} examples; max_len={self.max_len}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # возвращаем последовательности\n",
        "        return torch.tensor(self.x[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.long)\n",
        "\n",
        "# --- Модель: BiLSTM с параметром dropout ---\n",
        "class BiLSTMNER(nn.Module):\n",
        "    \"\"\"\n",
        "    BiLSTM модель с опциональным dropout.\n",
        "    Параметры: vocab_size, emb_dim, hidden_dim, num_labels, dropout\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim, num_labels, pad_idx=0, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(input_size=emb_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout) if dropout and dropout > 0 else nn.Identity()\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_labels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        emb = self.dropout(emb)\n",
        "        out, _ = self.lstm(emb)\n",
        "        out = self.dropout(out)\n",
        "        logits = self.fc(out)  # [B, T, num_labels]\n",
        "        return logits\n",
        "\n",
        "# --- compute_metrics: извлекаем метрики per-class и summary ---\n",
        "def compute_metrics(y_true_seqs, y_pred_seqs, idx2label, ignore_index):\n",
        "    \"\"\"\n",
        "    y_true_seqs, y_pred_seqs: списки списков индексов (int) с одинаковой длиной каждой подсписки.\n",
        "    ignore_index: индекс, который нужно игнорировать (паддинг)\n",
        "    Возвращает: metrics_by_class (dict), summary (dict)\n",
        "    \"\"\"\n",
        "    flat_true = []\n",
        "    flat_pred = []\n",
        "\n",
        "    for t_seq, p_seq in zip(y_true_seqs, y_pred_seqs):\n",
        "        # если вход — тензор/np, приводим к list\n",
        "        for t, p in zip(t_seq, p_seq):\n",
        "            if t == ignore_index:\n",
        "                continue\n",
        "            flat_true.append(int(t))\n",
        "            flat_pred.append(int(p))\n",
        "\n",
        "    if len(flat_true) == 0:\n",
        "        # ничего не осталось для оценки\n",
        "        return {}, {\n",
        "            \"macro_precision\": 0.0, \"macro_recall\": 0.0, \"macro_f1\": 0.0,\n",
        "            \"micro_precision\": 0.0, \"micro_recall\": 0.0, \"micro_f1\": 0.0\n",
        "        }\n",
        "\n",
        "    labels = sorted(list(set(flat_true)))\n",
        "    p, r, f1, support = precision_recall_fscore_support(flat_true, flat_pred, labels=labels, zero_division=0)\n",
        "\n",
        "    metrics_by_class = {}\n",
        "    for idx, lab in enumerate(labels):\n",
        "        label_name = idx2label.get(lab, str(lab))\n",
        "        metrics_by_class[label_name] = {\n",
        "            \"precision\": float(p[idx]),\n",
        "            \"recall\": float(r[idx]),\n",
        "            \"f1\": float(f1[idx]),\n",
        "            \"support\": int(support[idx])\n",
        "        }\n",
        "\n",
        "    macro_p, macro_r, macro_f1, _ = precision_recall_fscore_support(flat_true, flat_pred, average=\"macro\", zero_division=0)\n",
        "    micro_p, micro_r, micro_f1, _ = precision_recall_fscore_support(flat_true, flat_pred, average=\"micro\", zero_division=0)\n",
        "\n",
        "    summary = {\n",
        "        \"macro_precision\": float(macro_p),\n",
        "        \"macro_recall\": float(macro_r),\n",
        "        \"macro_f1\": float(macro_f1),\n",
        "        \"micro_precision\": float(micro_p),\n",
        "        \"micro_recall\": float(micro_r),\n",
        "        \"micro_f1\": float(micro_f1),\n",
        "        \"support_total\": int(sum(support))\n",
        "    }\n",
        "\n",
        "    return metrics_by_class, summary\n",
        "\n",
        "# --- train_model: отдельная функция обучения ---\n",
        "def train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    idx2label,\n",
        "    device,\n",
        "    epochs=10,\n",
        "    lr=1e-3,\n",
        "    load_pretrained=False,\n",
        "    pretrained_path=None,\n",
        "    save_best_path=None,\n",
        "    ignore_index=-100\n",
        "):\n",
        "    \"\"\"\n",
        "    Обучает модель на train_loader.\n",
        "    Параметры:\n",
        "      - model: nn.Module\n",
        "      - train_loader: DataLoader\n",
        "      - idx2label: dict (int->label_name)\n",
        "      - device: torch.device\n",
        "      - epochs, lr: гиперпараметры\n",
        "      - load_pretrained: если True, загружает веса из pretrained_path\n",
        "      - save_best_path: если задан, сохраняет лучший checkpoint\n",
        "      - ignore_index: индекс паддинга/игнорирования\n",
        "    Возвращает:\n",
        "      history (dict), final_metrics_by_class (dict), final_summary (dict)\n",
        "    \"\"\"\n",
        "    if load_pretrained:\n",
        "        assert pretrained_path is not None\n",
        "        model.load_state_dict(torch.load(pretrained_path, map_location=device))\n",
        "        print(f\"[train_model] Pretrained weights loaded from {pretrained_path}\")\n",
        "\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
        "\n",
        "    history = {\"loss\": [], \"macro_f1\": [], \"micro_f1\": []}\n",
        "    best_macro_f1 = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        all_true = []\n",
        "        all_pred = []\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=False)\n",
        "        for x_batch, y_batch in pbar:\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x_batch)  # [B, T, C]\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "            loss = criterion(logits.view(-1, C), y_batch.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # собираем предсказания для метрик\n",
        "            preds = torch.argmax(logits, dim=-1).cpu().numpy().tolist()\n",
        "            trues = y_batch.cpu().numpy().tolist()\n",
        "            all_pred.extend(preds)\n",
        "            all_true.extend(trues)\n",
        "\n",
        "            pbar.set_postfix(loss=f\"{loss.item():.6f}\")\n",
        "\n",
        "        # вычисляем метрики по эпохе\n",
        "        metrics_by_class, summary = compute_metrics(all_true, all_pred, idx2label, ignore_index)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        history[\"loss\"].append(epoch_loss)\n",
        "        history[\"macro_f1\"].append(summary[\"macro_f1\"])\n",
        "        history[\"micro_f1\"].append(summary[\"micro_f1\"])\n",
        "\n",
        "        # печать резюме по эпохе\n",
        "        print(f\"Epoch {epoch:03d} | loss={epoch_loss:.6f} | macro_f1={summary['macro_f1']:.4f} | micro_f1={summary['micro_f1']:.4f}\")\n",
        "\n",
        "        # сохранение лучшей модели по macro_f1\n",
        "        if save_best_path is not None:\n",
        "            if summary[\"macro_f1\"] > best_macro_f1:\n",
        "                best_macro_f1 = summary[\"macro_f1\"]\n",
        "                best_state = model.state_dict()\n",
        "                torch.save(best_state, save_best_path)\n",
        "                print(f\"[train_model] New best macro_f1={best_macro_f1:.4f} saved to {save_best_path}\")\n",
        "\n",
        "    # финальная оценка и возврат\n",
        "    final_metrics_by_class, final_summary = metrics_by_class, summary\n",
        "    return history, final_metrics_by_class, final_summary\n",
        "\n",
        "# --- plot_history: рисует loss и f1 кривые ---\n",
        "def plot_history(history, label=None):\n",
        "    \"\"\"\n",
        "    history: dict с ключами 'loss', 'macro_f1', 'micro_f1'\n",
        "    label: имя эксперимента для подписи кривых\n",
        "    \"\"\"\n",
        "    epochs = list(range(1, len(history[\"loss\"]) + 1))\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(epochs, history[\"loss\"], marker='o', label=label)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training loss\")\n",
        "    plt.grid(True)\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(epochs, history[\"macro_f1\"], marker='o', label=f\"{label} macro_f1\")\n",
        "    plt.plot(epochs, history[\"micro_f1\"], marker='x', label=f\"{label} micro_f1\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.title(\"F1 scores\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --- Сборщик таблицы сравнения ---\n",
        "def build_comparison_table(results_list):\n",
        "    \"\"\"\n",
        "    results_list: список словарей, где каждый словарь содержит:\n",
        "     - 'experiment', 'macro_f1', 'micro_f1', и при желании per-class metrics.\n",
        "    Возвращает pandas.DataFrame аккуратно оформленный.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for res in results_list:\n",
        "        row = {\n",
        "            \"experiment\": res.get(\"experiment\"),\n",
        "            \"macro_f1\": res.get(\"macro_f1\"),\n",
        "            \"micro_f1\": res.get(\"micro_f1\"),\n",
        "            \"macro_precision\": res.get(\"macro_precision\"),\n",
        "            \"macro_recall\": res.get(\"macro_recall\")\n",
        "        }\n",
        "        rows.append(row)\n",
        "    df = pd.DataFrame(rows).sort_values(by=\"macro_f1\", ascending=False).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "# --- Загрузка vocabs и подготовка dataloader (по sample) ---\n",
        "char2idx, label2idx, idx2label, pad_token_id, pad_label_id = load_vocabs()\n",
        "print(\"pad_token_id:\", pad_token_id, \"pad_label_id:\", pad_label_id)\n",
        "\n",
        "dataset = BiLSTMIndexedDataset(\"data/xTrain_BiLSTM_idx_5000.txt\", \"data/yTrain_BiLSTM_padded_5000.txt\")\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "print(\"Pipeline ready. Use train_model(...) to run experiments.\")\n"
      ],
      "metadata": {
        "id": "ErMC2I3WVsL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Этап 2. Experiment A (Baseline)."
      ],
      "metadata": {
        "id": "xkA29troWFXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# ЯЧЕЙКА 2: Experiment A — Baseline\n",
        "# - BiLSTM hidden_dim=128, emb=128, dropout=0.0\n",
        "# - вывод прогресса и метрик\n",
        "# ==========================\n",
        "# Параметры эксперимента\n",
        "exp_name = \"baseline_hidden128_emb128\"\n",
        "vocab_size = len(char2idx)\n",
        "num_labels = len(label2idx)\n",
        "emb_dim = 128\n",
        "hidden_dim = 128\n",
        "dropout = 0.0\n",
        "epochs = 8   # для демонстрации; если нужно больше, увеличьте\n",
        "lr = 1e-3\n",
        "save_path = f\"checkpoints/{exp_name}.pt\"\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "# Создаём модель\n",
        "model_a = BiLSTMNER(vocab_size=vocab_size, emb_dim=emb_dim, hidden_dim=hidden_dim, num_labels=num_labels, pad_idx=pad_token_id, dropout=dropout)\n",
        "\n",
        "# Обучаем\n",
        "history_a, metrics_by_class_a, summary_a = train_model(\n",
        "    model=model_a,\n",
        "    train_loader=train_loader,\n",
        "    idx2label=idx2label,\n",
        "    device=DEVICE,\n",
        "    epochs=epochs,\n",
        "    lr=lr,\n",
        "    load_pretrained=False,\n",
        "    pretrained_path=None,\n",
        "    save_best_path=save_path,\n",
        "    ignore_index=pad_label_id\n",
        ")\n",
        "\n",
        "# Сохраним результаты для последующего сравнения\n",
        "result_a = {\n",
        "    \"experiment\": exp_name,\n",
        "    \"history\": history_a,\n",
        "    \"metrics_by_class\": metrics_by_class_a,\n",
        "    \"macro_f1\": summary_a[\"macro_f1\"],\n",
        "    \"micro_f1\": summary_a[\"micro_f1\"],\n",
        "    \"macro_precision\": summary_a[\"macro_precision\"],\n",
        "    \"macro_recall\": summary_a[\"macro_recall\"]\n",
        "}\n",
        "print(\"Experiment A finished:\", result_a[\"experiment\"])\n"
      ],
      "metadata": {
        "id": "XC5QGIrPWIa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Этап 3. Experiment B (hidden_dim=64)."
      ],
      "metadata": {
        "id": "PX6azXXSWY9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# ЯЧЕЙКА 3: Experiment B — hidden_dim=64 (меньше параметров)\n",
        "# ==========================\n",
        "exp_name = \"hidden64_emb128\"\n",
        "vocab_size = len(char2idx)\n",
        "num_labels = len(label2idx)\n",
        "emb_dim = 128\n",
        "hidden_dim = 64\n",
        "dropout = 0.0\n",
        "epochs = 8\n",
        "lr = 1e-3\n",
        "save_path = f\"checkpoints/{exp_name}.pt\"\n",
        "\n",
        "model_b = BiLSTMNER(vocab_size=vocab_size, emb_dim=emb_dim, hidden_dim=hidden_dim, num_labels=num_labels, pad_idx=pad_token_id, dropout=dropout)\n",
        "\n",
        "history_b, metrics_by_class_b, summary_b = train_model(\n",
        "    model=model_b,\n",
        "    train_loader=train_loader,\n",
        "    idx2label=idx2label,\n",
        "    device=DEVICE,\n",
        "    epochs=epochs,\n",
        "    lr=lr,\n",
        "    load_pretrained=False,\n",
        "    pretrained_path=None,\n",
        "    save_best_path=save_path,\n",
        "    ignore_index=pad_label_id\n",
        ")\n",
        "\n",
        "result_b = {\n",
        "    \"experiment\": exp_name,\n",
        "    \"history\": history_b,\n",
        "    \"metrics_by_class\": metrics_by_class_b,\n",
        "    \"macro_f1\": summary_b[\"macro_f1\"],\n",
        "    \"micro_f1\": summary_b[\"micro_f1\"],\n",
        "    \"macro_precision\": summary_b[\"macro_precision\"],\n",
        "    \"macro_recall\": summary_b[\"macro_recall\"]\n",
        "}\n",
        "print(\"Experiment B finished:\", result_b[\"experiment\"])\n"
      ],
      "metadata": {
        "id": "u6pORa4mWhB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Этап 4. Experiment C (Dropout)."
      ],
      "metadata": {
        "id": "Mvf74uZ8WiEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# ЯЧЕЙКА 4: Experiment C — Dropout (dropout=0.3)\n",
        "# ==========================\n",
        "exp_name = \"dropout03_hidden128\"\n",
        "vocab_size = len(char2idx)\n",
        "num_labels = len(label2idx)\n",
        "emb_dim = 128\n",
        "hidden_dim = 128\n",
        "dropout = 0.3\n",
        "epochs = 8\n",
        "lr = 1e-3\n",
        "save_path = f\"checkpoints/{exp_name}.pt\"\n",
        "\n",
        "model_c = BiLSTMNER(vocab_size=vocab_size, emb_dim=emb_dim, hidden_dim=hidden_dim, num_labels=num_labels, pad_idx=pad_token_id, dropout=dropout)\n",
        "\n",
        "history_c, metrics_by_class_c, summary_c = train_model(\n",
        "    model=model_c,\n",
        "    train_loader=train_loader,\n",
        "    idx2label=idx2label,\n",
        "    device=DEVICE,\n",
        "    epochs=epochs,\n",
        "    lr=lr,\n",
        "    load_pretrained=False,\n",
        "    pretrained_path=None,\n",
        "    save_best_path=save_path,\n",
        "    ignore_index=pad_label_id\n",
        ")\n",
        "\n",
        "result_c = {\n",
        "    \"experiment\": exp_name,\n",
        "    \"history\": history_c,\n",
        "    \"metrics_by_class\": metrics_by_class_c,\n",
        "    \"macro_f1\": summary_c[\"macro_f1\"],\n",
        "    \"micro_f1\": summary_c[\"micro_f1\"],\n",
        "    \"macro_precision\": summary_c[\"macro_precision\"],\n",
        "    \"macro_recall\": summary_c[\"macro_recall\"]\n",
        "}\n",
        "print(\"Experiment C finished:\", result_c[\"experiment\"])\n"
      ],
      "metadata": {
        "id": "EAZ-0buYWnYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Этап 5. Графики трех экспериментов и итоговая таблица сравнения"
      ],
      "metadata": {
        "id": "lmVIzmROWu8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# ЯЧЕЙКА 5: Сравнение: графики + таблица\n",
        "# Требование: вывод macro/micro F1 и per-class метрик в таблице\n",
        "# ==========================\n",
        "# Собираем результаты (предполагается, что переменные result_a, result_b, result_c существуют)\n",
        "all_results = []\n",
        "for res in [result_a, result_b, result_c]:\n",
        "    all_results.append({\n",
        "        \"experiment\": res[\"experiment\"],\n",
        "        \"macro_f1\": res[\"macro_f1\"],\n",
        "        \"micro_f1\": res[\"micro_f1\"],\n",
        "        \"macro_precision\": res.get(\"macro_precision\", None),\n",
        "        \"macro_recall\": res.get(\"macro_recall\", None)\n",
        "    })\n",
        "\n",
        "# Построим объединённые графики: loss и macro/micro F1\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Loss subplot\n",
        "plt.subplot(1,2,1)\n",
        "for res in [result_a, result_b, result_c]:\n",
        "    plt.plot(range(1, len(res[\"history\"][\"loss\"]) + 1), res[\"history\"][\"loss\"], marker='o', label=res[\"experiment\"])\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss (training)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Macro/Micro F1 subplot\n",
        "plt.subplot(1,2,2)\n",
        "for res in [result_a, result_b, result_c]:\n",
        "    epochs = range(1, len(res[\"history\"][\"macro_f1\"]) + 1)\n",
        "    plt.plot(epochs, res[\"history\"][\"macro_f1\"], marker='o', label=f\"{res['experiment']} macro_f1\")\n",
        "    plt.plot(epochs, res[\"history\"][\"micro_f1\"], marker='x', linestyle='--', label=f\"{res['experiment']} micro_f1\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"F1 scores\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Таблица сравнения (summary)\n",
        "df = build_comparison_table(all_results)\n",
        "print(\"=== Summary table ===\")\n",
        "display(df)\n",
        "\n",
        "# Per-class таблицы: показываем top-rows для каждого эксперимента\n",
        "for res in [result_a, result_b, result_c]:\n",
        "    print(f\"\\n=== Per-class metrics: {res['experiment']} ===\")\n",
        "    if res[\"metrics_by_class\"]:\n",
        "        per_class_df = pd.DataFrame.from_dict(res[\"metrics_by_class\"], orient=\"index\").sort_values(by=\"support\", ascending=False)\n",
        "        display(per_class_df)\n",
        "    else:\n",
        "        print(\"No per-class metrics available.\")\n"
      ],
      "metadata": {
        "id": "JuVFsH01W2Xv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "fptCPUtZbNKA",
        "Fzc-1N2BZDyy",
        "afuIcuVZZMBI",
        "9peYOsQVZoW7",
        "M42mZHXqZ89W",
        "G4ld2kecYrFw",
        "KIdCAOsbcNNX",
        "6FI9t9jAY1XD"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}